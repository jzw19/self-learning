# ğŸŒ Primer on Large Scale Machine Learning

Large datasets can make training too slow if we rely on â€œvanillaâ€ algorithms.  
This module shows techniques to make learning **practical and scalable**.

---

## Core Ideas
1. **Donâ€™t use the full dataset for every step** â†’ use stochastic or mini-batch updates.  
2. **Exploit parallelism** â†’ distribute heavy computations.  
3. **Think incrementally** â†’ update as new data arrives.  
4. **Use learning curves** â†’ figure out whether to get more data or improve the model.  

---

## Intuition
- Batch GD is like taking careful, precise steps toward the minimum â€” slow but steady.  
- SGD is like running downhill with some randomness â€” noisy, but youâ€™ll get close fast.  
- Mini-batch GD is the â€œsweet spotâ€ between them.  

---

## When to Use What
- **SGD**: When dataset is too large to fit in memory or when data is streaming.  
- **Mini-batch GD**: Default for large datasets with good hardware (GPU/CPU parallelism).  
- **Online learning**: Continuous, never-ending data (e.g., recommendation engines).  
- **MapReduce/Distributed**: Web-scale data, beyond a single machine.  
