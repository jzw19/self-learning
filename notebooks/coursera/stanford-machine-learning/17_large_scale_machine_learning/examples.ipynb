{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš¡ Large Scale Machine Learning Examples\n",
    "\n",
    "This notebook demonstrates **Batch Gradient Descent**, **Stochastic Gradient Descent (SGD)**, and **Mini-batch Gradient Descent** using a simple linear regression problem.\n",
    "\n",
    "We will:\n",
    "- Generate synthetic data\n",
    "- Implement each gradient descent variant\n",
    "- Compare convergence behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic linear data: y = 4x + 3 + noise\n",
    "np.random.seed(42)\n",
    "m = 200  # number of examples\n",
    "X = 2 * np.random.rand(m, 1)\n",
    "y = 4 * X + 3 + np.random.randn(m, 1)\n",
    "\n",
    "# Add bias term (x0 = 1)\n",
    "X_b = np.c_[np.ones((m, 1)), X]  # shape (m, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X, y, alpha=0.1, n_iters=100):\n",
    "    m = len(y)\n",
    "    theta = np.random.randn(2,1)\n",
    "    cost_history = []\n",
    "    for i in range(n_iters):\n",
    "        gradients = (2/m) * X.T @ (X @ theta - y)\n",
    "        theta -= alpha * gradients\n",
    "        cost = (1/m) * np.sum((X @ theta - y) ** 2)\n",
    "        cost_history.append(cost)\n",
    "    return theta, cost_history\n",
    "\n",
    "theta_bgd, cost_bgd = batch_gradient_descent(X_b, y)\n",
    "print(\"Batch GD theta:\", theta_bgd.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, alpha=0.1, n_epochs=50):\n",
    "    m = len(y)\n",
    "    theta = np.random.randn(2,1)\n",
    "    cost_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(m):\n",
    "            rand_i = np.random.randint(m)\n",
    "            xi = X[rand_i:rand_i+1]\n",
    "            yi = y[rand_i:rand_i+1]\n",
    "            gradients = 2 * xi.T @ (xi @ theta - yi)\n",
    "            theta -= alpha * gradients\n",
    "        cost = (1/m) * np.sum((X @ theta - y) ** 2)\n",
    "        cost_history.append(cost)\n",
    "    return theta, cost_history\n",
    "\n",
    "theta_sgd, cost_sgd = stochastic_gradient_descent(X_b, y)\n",
    "print(\"SGD theta:\", theta_sgd.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent(X, y, alpha=0.1, n_iters=50, batch_size=20):\n",
    "    m = len(y)\n",
    "    theta = np.random.randn(2,1)\n",
    "    cost_history = []\n",
    "    for it in range(n_iters):\n",
    "        indices = np.random.permutation(m)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        for i in range(0, m, batch_size):\n",
    "            xi = X_shuffled[i:i+batch_size]\n",
    "            yi = y_shuffled[i:i+batch_size]\n",
    "            gradients = (2/batch_size) * xi.T @ (xi @ theta - yi)\n",
    "            theta -= alpha * gradients\n",
    "        cost = (1/m) * np.sum((X @ theta - y) ** 2)\n",
    "        cost_history.append(cost)\n",
    "    return theta, cost_history\n",
    "\n",
    "theta_mbgd, cost_mbgd = minibatch_gradient_descent(X_b, y)\n",
    "print(\"Mini-batch GD theta:\", theta_mbgd.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(cost_bgd, label=\"Batch GD\")\n",
    "plt.plot(cost_sgd, label=\"SGD\")\n",
    "plt.plot(cost_mbgd, label=\"Mini-batch GD\")\n",
    "plt.xlabel(\"Iteration/Epoch\")\n",
    "plt.ylabel(\"Cost (MSE)\")\n",
    "plt.title(\"Convergence of Gradient Descent Variants\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
